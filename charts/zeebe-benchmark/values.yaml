# Default values for Zeebe Benchmark Helm chart.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# The values file follows helm best practices https://helm.sh/docs/chart_best_practices/values/
#
# This means:
#   * Variable names should begin with a lowercase letter, and words should be separated with camelcase.
#   * Every defined property in values.yaml should be documented. The documentation string should begin with the name of the property that it describes, and then give at least a one-sentence description
#
# Furthermore, we try to apply the following pattern: # [VarName] [conjunction] [definition]
#
# VarName:
#
#  * In the documentation the variable name is started with a big letter, similar to kubernetes resource documentation.
#  * If the variable is part of a subsection/object we use a json path expression (to make it more clear where the variable belongs to).
#    The root (chart name) is omitted (e.g. zeebe). This is useful for using --set in helm.
#
# Conjunction:
#   * [defines] for mandatory configuration
#   * [can be used] for optional configuration
#   * [if true] for toggles
#   * [configuration] for section/group of variables

# Global configuration for variables which can be accessed by all sub charts
global:
  # Disable global ingress
  ingress:
    enabled: false
  image:
    # Image.repository defines the repository from which to fetch the docker images
    repository: "gcr.io/zeebe-io"
    # Image.tag defines the tag / version which should be used in the chart
    tag: SNAPSHOT
    ## @param global.image.pullPolicy defines the image pull policy which should be used https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy
    pullPolicy: Always
    ## @param global.image.pullSecrets can be used to configure image pull secrets https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod
    pullSecrets: [ ]
  # Identity configuration to configure identity specifics on global level, which can be accessed by other sub-charts
  identity:
    auth:
      enabled: false

identity:
  enabled: false

# Saas configuration to run benchmarks against Camunda SaaS environment
saas:
  # Saas.enabled if true enables the benchmark to run against Camunda SaaS
  enabled: false

  # Saas.credentials configuration to connect to a Camunda SaaS cluster
  credentials:
    # Saas.existingSecret can be used to configure the secret name that should be referenced by the benchmark
    # applications to retrieve credential information.
    #
    # If this value is set, other credentials are not used.
    #
    # Credentials Secret need to follow the following format:
    #
    # apiVersion: v1
    # kind: Secret
    # metadata:
    #   name: cloud-credentials
    # type: Opaque
    # stringData:
    #   clientId: hH55UFivfw-bbHAuPwN545oyv8tTdW0z
    #   clientSecret: xtHQB.zBLcQrw4GaP0k_ci~ePjbD8qVlYaFKNo__2a7ZJxL-DAVVHepq~X9elPRb
    #   zeebeAddress: e314a337-a462-4988-a3be-d1f2e153e034.zeebe.ultrawombat.com:443
    #   authServer: https://login.cloud.ultrawombat.com/oauth/token
    existingSecret:

    # Saas.credentials.clientId to define the clientId to connect
    clientId: ""
    # Saas.credentials.clientSecret to define the clientSecret to connect
    clientSecret: ""
    # Saas.credentials.zeebeAddress to define the address of the cluster (including port)
    zeebeAddress:
    # SaaS.credentials.authServer to define the authentication server to retrieve JWT tokens
    authServer: "https://login.cloud.ultrawombat.com/oauth/token"

# Workers configuration for the to be deployed worker application
#        => New way to deploy workers <=
workers:
  # Workers.benchmark defines the configuration for the default benchmark worker
  # See below if you want additional workers
  image:
    repository: "gcr.io/zeebe-io"
    tag: SNAPSHOT
  benchmark:
    # Workers.benchmark.replicas defines how many replicas of the benchmark worker should be deployed
    replicas: 3
    # Workers.benchmark.capacity defines how many jobs the worker should activate and work on
    capacity: 60
    # Workers.benchmark.threads defines how many threads the worker can use to work on jobs
    threads: 10
    # Workers.benchmark.jobType defines the job type which should be used by the worker
    jobType: "benchmark-task"
    # Workers.benchmark.payloadPath defines the path (inside the worker app) to the payload resource
    # that should be used to complete the corresponding jobs
    payloadPath: "bpmn/big_payload.json"
    # Workers.benchmark.payloadPath defines the delay of the worker before completing a job
    completionDelay: 50ms
    # Workers.benchmark.logLevel defines the logging level for the benchmark worker
    logLevel: "WARN"
    # Workers.benchmark.resources defines the resources for the benchmark worker
    resources:
      limits:
        cpu: 500m
        memory: 256Mi
      requests:
        cpu: 500m
        memory: 256Mi

  # Workers.benchmark defines the configuration for the default benchmark worker
  # Adding more and different worker can be done via adding a new map like:
  #  benchmark-2:
  #    # Workers.benchmark.replicas defines how many replicas of the benchmark worker should be deployed
  #    replicas: 3
  #    # Workers.benchmark.capacity defines how many jobs the worker should activate and work on
  #    capacity: 60
  #    # Workers.benchmark.jobType defines the job type which should be used by the worker
  #    jobType: "benchmark-2-task"
  #    # Workers.benchmark.logLevel defines the logging level for the benchmark worker
  #    logLevel: "WARN"
  #    # Workers.benchmark.resources defines the resources for the benchmark worker
  #    resources:
  #      limits:
  #        cpu: 500m
  #        memory: 256Mi
  #      requests:
  #        cpu: 500m
  #        memory: 256Mi

# Starter configuration for the to be deployed starter application
starter:
  image:
    repository: "gcr.io/zeebe-io"
    tag: SNAPSHOT
  # Starter.replicas defines how many replicas of the application should be deployed
  replicas: 1
  # Starter.rate defines with which rate process instances should be created by the starter
  rate: 150
  # Starter.logLevel defines the logging level for the benchmark starter
  logLevel: "WARN"
  # Starter.processId defines the process ID, that should be used for creating new process instances
  processId: "benchmark"
  # Starter.payloadPath defines the path (inside the starter app) to the payload resource
  # that should be used to create the corresponding process instance
  payloadPath: "bpmn/big_payload.json"
  # Starter.bpmnXmlPath defines the path (inside the starter app) to the main bpmn XML resource that should be deployed
  bpmnXmlPath: "bpmn/one_task.bpmn"
  # Starter.extraBpmnModels can be used to specify paths (inside the starter app) to extra resources that should be deployed
  extraResources: []
  # Starter.businessKey can be used to specify a businessKey variable, inside a unique identifier is stored for
  # each created process instance
  businessKey: "businessKey"

# Publisher configuration for the to be deployed publisher application
publisher:
  # Publisher.replicas defines how many replicas of the application should be deployed
  replicas: 0
  # Publisher.rate defines with which rate message should be published
  rate: 25

# Timer configuration for the to be deployed timer application
timer:
  # Timer.replicas defines how many replicas of the application should be deployed
  replicas: 0
  # Timer.rate defines with which rate process instances with timers should be created
  rate: 25

# LeaderBalancing configuration for the auto rebalancing feature, which allows to rebalance periodically the zeebe cluster
# For more details see https://docs.camunda.io/docs/self-managed/zeebe-deployment/operations/rebalancing/
leaderBalancing:
  # LeaderBalancing.enabled if true, enables the auto leader rebalancing
  enabled: true
  # LeaderBalancing.schedule defines the schedule of the auto leader rebalancing feature.
  schedule: "*/15 * * * *"

# Zeebe configuration to configure Zeebe and Gateway
zeebe:
  # Zeebe.config can be used to configure Zeebe Broker and Gateway additional without the need of overwriting all
  # environment variables from the dependency chart.
  # Allows to set configurations via --set, like --set zeebe.config.zeebe.broker.cluster.replicationFactor=3
  config:
    zeebe.gateway.monitoring.enabled: "true"
    zeebe.gateway.threads.managementThreads: "1"
    zeebe.broker.experimental.consistencyChecks.enablePreconditions: "true"
    zeebe.broker.experimental.consistencyChecks.enableForeignKeyChecks: "true"
    zeebe.broker.executionMetricsExporterEnabled: "true"
    zeebe.broker.data.diskUsageCommandWatermark: "0.8"
    zeebe.broker.data.diskUsageReplicationWatermark: "0.9"
    # Configure index suffix with hour pattern, so we create every hour a new index
    # such that ILM can clean it up quickly
    zeebe.broker.exporters.elasticsearch.args.index.indexSuffixDatePattern: "yyyy-MM-dd_HH"
    # Configure a rate limit for all writes, so that we can visualize flow control metrics.
    zeebe.broker.flowControl.write.enabled: "true"
    zeebe.broker.flowControl.write.limit: 4000
  # Zeebe.profiling configuration for pyroscope profiling
  profiling:
    # Zeebe.profiling.enabled if true, enables the pyroscope profiling
    enabled: false

camunda-platform:
  identity:
    enabled: false

  identityKeycloak:
    enabled: false

  optimize:
    enabled: false

  connectors:
    enabled: false

  webModeler:
    enabled: false

  postgresql:
    enabled: false

  core:
    ## @param core.clusterSize defines the amount of brokers (=replicas), which are deployed via helm
    clusterSize: 3
    ## @param core.partitionCount defines how many partitions are set up in the cluster
    partitionCount: 3
    ## @param core.replicationFactor defines how each partition is replicated, the value defines the number of nodes
    replicationFactor: 3
    ## @param core.cpuThreadCount defines how many threads can be used for the processing on each broker pod
    cpuThreadCount: 3
    ## @param core.ioThreadCount defines how many threads can be used for the exporting on each broker pod
    ioThreadCount: 3
    ## @extra core.resources configuration to set request and limit configuration for the container https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits
    ## @extra core.resources.requests
    ## @param core.resources.requests.cpu
    ## @param core.resources.requests.memory
    ## @param core.resources.limits.cpu
    ## @param core.resources.limits.memory
    resources:
      requests:
        cpu: 2000m
        memory: 6Gi
      limits:
        cpu: 2000m
        memory: 12Gi
    ## @param core.pvcSize defines the persistent volume claim size, which is used by each broker pod https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims
    pvcSize: "64Gi"
    # @extra core.containerSecurityContext defines the security options the container should be run with
    containerSecurityContext:
      ## @param core.containerSecurityContext.allowPrivilegeEscalation
      allowPrivilegeEscalation: true
      ## @param core.containerSecurityContext.privileged
      privileged: true
      ## @param core.containerSecurityContext.runAsNonRoot
      runAsNonRoot: true
      ## @param core.containerSecurityContext.runAsUser
      runAsUser: 1000
      capabilities:
        add: [ "NET_ADMIN" ]
    javaOpts: >-
      -XX:MaxRAMPercentage=25.0
      -XX:+ExitOnOutOfMemoryError
      -XX:+HeapDumpOnOutOfMemoryError
      -XX:HeapDumpPath=/usr/local/camunda/data
      -XX:ErrorFile=/usr/local/camunda/data/zeebe_error%p.log
      -Xlog:gc*:file=/usr/local/camunda/data/gc.log:time:filecount=7,filesize=8M

    # Zeebe config
    extraVolumes:
      - name: benchmark-config
        configMap:
          name: benchmark-config
          defaultMode: 0754
      - name: pyroscope
        emptyDir: { }
    ## @param core.extraVolumeMounts can be used to mount extra volumes for the broker pods, useful for additional exporters
    extraVolumeMounts:
      - name: benchmark-config
        mountPath: /usr/local/camunda/config/benchmark-config.yaml
        subPath: benchmark-config.yaml
      - name: pyroscope
        mountPath: /pyroscope
    ## @param core.extraInitContainers (Deprecated - use `initContainers` instead) ExtraInitContainers can be used to set up extra init containers for the broker pods, useful for additional exporters
    extraInitContainers:
      - name: pyroscope
        image: alpine
        command: [ 'wget', 'https://github.com/pyroscope-io/pyroscope-java/releases/latest/download/pyroscope.jar', '-O', '/pyroscope/pyroscope.jar' ]
        volumeMounts:
          - name: pyroscope
            mountPath: /pyroscope
        # We have to configure the security context to not run as Root
        securityContext:
          ## @param securityContext.allowPrivilegeEscalation
          allowPrivilegeEscalation: false
          ## @param securityContext.privileged
          privileged: false
          ## @param securityContext.readOnlyRootFilesystem
          readOnlyRootFilesystem: true
          ## @param securityContext.runAsUser
          runAsUser: 1000
    retention:
      ## @param core.retention.enabled if true, the ILM Policy is created and applied to the index templates.
      enabled: true
      ## @param core.retention.minimumAge defines how old the data must be, before the data is deleted as a duration.
      minimumAge: 10m
    # @param core.env can be used to set extra environment variables in each broker container
    env:
      - name: K8S_NAMESPACE
        valueFrom:
          fieldRef:
            fieldPath: metadata.namespace
      - name: K8S_NAME
        valueFrom:
          fieldRef:
            fieldPath: metadata.name
      # Enable JSON logging for google cloud stackdriver
      - name: ZEEBE_LOG_APPENDER
        value: Stackdriver
      - name: ZEEBE_LOG_STACKDRIVER_SERVICENAME
        value: zeebe
      - name: ZEEBE_LOG_STACKDRIVER_SERVICEVERSION
        valueFrom:
          fieldRef:
            fieldPath: metadata.namespace
      - name: ATOMIX_LOG_LEVEL
        value: INFO
      - name: ZEEBE_LOG_LEVEL
        value: DEBUG
      - name: PYROSCOPE_SERVER_ADDRESS
        value: "http://pyroscope.pyroscope.svc.cluster.local:4040"
      - name: PYROSCOPE_APPLICATION_NAME
        value: "io.camunda.zeebe.broker"
      - name: PYROSCOPE_LOG_LEVEL
        value: "debug"
      - name: PYROSCOPE_FORMAT
        value: "jfr"
      - name: PYROSCOPE_PROFILER_EVENT
        value: "cpu"
      - name: PYROSCOPE_PROFILER_ALLOC
        value: "0"
      - name: PYROSCOPE_PROFILER_LOCK
        value: "0"
      - name: PYROSCOPE_LABELS
        value: "namespace=$(K8S_NAMESPACE), pod=$(K8S_NAME)"
      # To be able to load a separate/additional configuration
      # See https://github.com/camunda/camunda-platform-helm/issues/2197
      - name: SPRING_CONFIG_ADDITIONALLOCATION
        value: "/usr/local/camunda/config/benchmark-config.yaml"
      - name: JAVA_OPTS
        valueFrom:
          configMapKeyRef:
            name: zeebe-config
            key: java-opts
            optional: true

  ############################################
  ## @section Elasticsearch Parameters
  ## @extra elasticsearch
  elasticsearch:
    master:
      ## @param elasticsearch.master.replicaCount defines number of master-elegible replicas to deploy
      replicaCount: 3
      ## @param elasticsearch.master.heapSize
      heapSize: 3g
      persistence:
        ## @param elasticsearch.master.persistence.size
        size: 64Gi
      resources:
        requests:
          cpu: 1
          memory: 3Gi
        limits:
          cpu: 2
          memory: 6Gi

  # Change these settings to configure a different way to collect metrics
  prometheusServiceMonitor:
    enabled: true
    labels:
      release: monitoring
    scrapeInterval: 30s

prometheus-elasticsearch-exporter:
  es:
    ## Address (host and port) of the Elasticsearch node we should connect to.
    ## This could be a local node (localhost:9200, for instance), or the address
    ## of a remote Elasticsearch server. When basic auth is needed,
    ## specify as: <proto>://<user>:<password>@<host>:<port>. e.g., http://admin:pass@localhost:9200.
    ##
    uri: "http://{{ .Release.Name }}-elasticsearch:9200"
  serviceMonitor:
    ## If true, a ServiceMonitor CRD is created for a prometheus operator
    ## https://github.com/coreos/prometheus-operator
    ##
    enabled: true
    # Labels used by the service monitor, specific to our prometheus installation
    labels:
      release: monitoring
